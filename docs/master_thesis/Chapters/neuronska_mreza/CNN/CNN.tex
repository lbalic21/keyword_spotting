\section{Konvolucijska neuronska mreža}
\label{sec:cnn}

Konvolucijska neuronska mreža (engl. \textit{Convolutional neural network ili CNN}) je vrsta 
umjetne neuronske mreže koja je pogodna je za obradu podataka s rešetkastom (matričnom)
topologijom, a najviše se koristi za rješavanje problema iz područja klasifikacije slika
te računalnog vida. Inspirirane su načinom funkcioniranja moždanog korteksa 
zaduženog za vid kod sisavaca \cite{pycodemates}. Obrada slike koju vide sisavci
funkcionira hijerarhijski, tj. u mozgu se ne obrađuje cjelokupna slika odjednom,
nego postoje jednostavnije stanice koje su zadužene za prepoznavanje osnovnijih
oblika koji se nakon toga stapaju u složenije. Naposljetku 
organizam može prepoznati cjelokupnu sliku koju gleda očima.
Veza između prepoznavanja slika i problema klasifikacije glasovnih naredbi možda nije
vidljiva na prvu. Međutim, generirana matrica značajki iz poglavlja \ref{sec:gen} predstavlja
upravo dvodimenzionalnu
sliku koja se može koristiti kao ulaz u konvolucijsku neuronsku mrežu.

Računalni modeli koji koriste strukturu sličnu opisanoj mogu iz
podataka koji su u takvom obliku izvući značajke samostalno što znači da nema
potrebe za korištenjem metoda koje eksplicitno izvlače bitne značajke iz matričnih podataka
\cite{1}. Arhitektura najjednostavnije konvolucijske neuronske mreže
uključuje:

\begin{itemize}
    \item \textbf{Ulaz}: ulazni sloj modela, prima matrični podatak
    \item \textbf{Konvolucijski sloj}: osnovni sloj modela. Njegov glavni zadatak
          je ekstrakcija značajki iz ulaznih podataka. Vidi \ref{sub:conv}.
    \item \textbf{Sloj za poduzorkovanje}: smanjuje dimenzionalnost (vidi \ref{sub:pooling})
    \item \textbf{Sloj za poravnavanje}: spaja konvolucijski i potpuno povezani dio (vidi \ref{sub:flat})
    \item \textbf{Potpuno povezani sloj}: povezuje značajke s klasifikacijom (vidi \ref{sub:dense})
    \item \textbf{Izlaz}: izlazni sloj, daje vjerojatnosti klasifikacije (vidi \ref{sub:output})
\end{itemize}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{Chapters/neuronska_mreza/CNN/cnn.png} 
    \caption{Jednostavna CNN \cite{1}}
    \label{pic:cnn}
\end{figure}

Na slici \ref{pic:cnn} prikazana je opisana struktura. Prva tri sloja grupirana su
u dio koji služi za ekstrakciju značajki iz ulaznih podataka, a posljednja dva
sloja služe za klasifikaciju. Složenije arhitekture mreže mogu imati veći broj
konvolucijskih slojeva (nakon svakog se nalazi sloj za poduzorkovanje) te veći broj
složenijih ili manje složenih potpuno povezanih slojeva.

\subsection{Konvolucijski sloj}
\label{sub:conv}

Najbitniji dio ovakvog tipa neuronske mreže je konvolucijski sloj 
(engl. \textit{convolutional layer}) zbog toga što se
u njemu događa konvolucija. Konvolucija (u neuronskim mrežama) je proces kojim se
iz ulazne matrice podataka (slike) na izlazu dobije matrica značajki ili
mapa značajki. Neka je ulazna matrica oblika \( x \in M_{mn}(\mathbb{R}) \).
Umjesto težina (kao kod potpuno povezanog sloja), konvolucijski sloj koristi
matricu \( \omega \in M_{pr}(\mathbb{R}) \) koju nazivamo filtar ili
jezgra (engl. \textit{kernel}) \cite{keras_layers}. Svaki konvolucijski 
sloj može imati proizvoljan broj filtara. Izlazna (u ovom slučaju dvodimenzionalna)
mapa značajki tada se računa na sljedeći način:

\begin{equation}
h = \omega * x,
\end{equation}

pri čemu \( * \) označava operaciju konvolucije te vrijedi:

\begin{equation}
h(i, j) = (\omega * x)(i, j) = 
\sum_{k=0}^{p-1} \sum_{l=0}^{r-1} x(i + k, j + l) \omega(k, l).
\end{equation}

Formula koja se zapravo koristi naziva se unakrsna korelacija, međutim, zbog sličnosti s 
formulom konvolucije, mreža nosi takav naziv \cite{gracan2020}. Na slici 
\ref{pic:convolution} prikazan je proces koji se događa tijekom prolaska 
ulaznog podatka kroz konvolucijski sloj. Ulaz (engl. \textit{input}) se konvolucijski množi
s jezgrom kako bi se dobila izlazna matrica značajki \cite{cnn_how}. U slučaju prikazanom na slici,
ulazna slika je veličine \(4 \times 4\), dok je jezgra veličine \(3 \times 3\). Prvo se podmatrica ulaznog
podatka veličine jednake veličini jezgre (\(3 \times 3\)) skalarno množi s jezgrom. Izlaz je 
skalarni umnožak na koji se može dodati konstantna vrijednost (engl. \textit{bias}). Nakon
toga se jezgra pomiče po ulaznoj matrici, tj. sljedeći element izlazne matrice je
skalarni umnožak jezgre i sljedeće podmatrice ulaznog podatka. Koliko će se jezgra
pomaknuti određuje pomak (engl. \textit{stride}). U slučaju na slici \ref{pic:convolution}
pomak iznosi jedan.

\begin{figure}[htb]
      \centering
      \includegraphics[width=0.5\linewidth]{Chapters/neuronska_mreza/CNN/convolution.png} 
      \caption{Konvolucija \cite{convolution}}
      \label{pic:convolution}
\end{figure}

Rezultat opisanog procesa je mapa značajki koja je manja od ulazne, a njena veličina
obrnuto proporcionalno ovisi o veličini jezgre te pomaku \cite{cnn_whatis}. 

Slično procesu koji se odvija u mozgu čovjeka, opisana struktura omogućava hijerarhijsko učenje.
Naime, svaki konvolucijski sloj sadrži jezgre koje su zadužene za lokalno pretraživanje
određenih uzoraka. Upravo konvolucijom izvlačimo stvari koje su slične između različitih
ulaznih slika. Koje jezgre se trebaju koristiti određuje proces učenja (treniranja
mreže) koji prepoznaje lokalne sličnosti između različitih primjera. Ako se mreža sastoji
od više uzastopnih konvolucijskih slojeva, prvo će naučiti najjednostavnije
oblike, a zatim u sljedećem sloju takvim oblicima slagati složenije uzorke. Također,
još jedna prednost konvolucijskog sloja je u dijeljenju parametara. Takav sloj nema vezu 
svakog neurona sa svakim ulaznim, nego se težine dijele unutar određene jezgre. Zapravo se
cijeli proces učenja svodi na nalaženje odgovarajućih jezgri koje će prepoznati uzorke \cite{pycodemates}.
U slučaju dvodimenzionalne ulazne slike, broj parametara ovakvog sloja iznosit će:

\begin{equation}
    N = (n \cdot m \cdot C_{\text{in}} + 1) \cdot C_{\text{out}}
\end{equation}

gdje su:
\begin{itemize}
    \item \(N\): ukupni broj parametara,
    \item \(n\) i \(m\): visina i širina filtra (jezgre),
    \item \(C_{\text{in}}\): broj ulaznih kanala (npr. 1 za crno-bijele slike, 3 za RGB slike),
    \item \(+1\): konstanta svakog filtra,
    \item \(C_{\text{out}}\): broj filtara (odnosno mapa izlaznih značajki).
\end{itemize}

Kako bi se bolje dočarala razlika u broju parametara između ovakvog sloja i potpuno povezanog sloja,
kao primjer može se uzeti slika \ref{pic:convolution}. Ulazna slika je veličine \(4 \times 4\),
jezgra \(3 \times 3\), a izlaz 
\(2 \times 2\). Neka je slika jednokanalna, a broj jezgri jedan (sve kao na slici). Konvolucijski sloj
će u tom slučaju imati 10 parametara, dok će potpuno povezani sloj (16 ulaznih vrijednosti, 4 izlazne te
4 konstante za svaki neuron) imati 68 parametara! Formula za broj parametara u tom slučaju
je sljedeća:

\begin{equation}
    N = n_{\text{in}} \cdot n_{\text{out}} + n_{\text{out}}
\end{equation}

gdje su:
\begin{itemize}
    \item \(N\): ukupni broj parametara,
    \item \(n_{\text{in}}\): broj ulaznih neurona,
    \item \(n_{\text{out}}\): broj izlaznih neurona.
\end{itemize}

Također, kao što je opisano u poglavlju \ref{pog:neuronska_mreza} vrijednost neurona (čvora) na
izlazu iz sloja potrebno je provući kroz aktivacijsku funkciju. Postoje različite vrste funkcija
koje se koriste u različitim granama strojnog i dubokog učenja \cite{activation_fcn}, a za
primjenu u konvolucijskim slojevima, najefektivnija se pokazala ReLu (engl. \textit{Rectified linear units})
\cite{relu}. To je funkcija koja vraća nulu ako joj je ulaz negativan, a za svaki pozitivan
ulaz samo prosljeđuje istu vrijednost na izlaz. Modelirana je formulama \ref{eq:relu1} i 
\ref{eq:relu2}, a prikazana je na slici \ref{pic:relu}. 

\begin{equation}
    f(x) = \max(0, x)
    \label{eq:relu1}
\end{equation}

\begin{equation}
    f(x) = 
    \begin{cases} 
        0, & \text{ako } x < 0, \\
        x, & \text{ako } x \geq 0.
    \end{cases}
    \label{eq:relu2}
\end{equation}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{Chapters/neuronska_mreza/CNN/relu.png} 
    \caption{ReLu \cite{relu}}
    \label{pic:relu}
\end{figure}

Prednosti ReLu funkcije nad drugim aktivacijskim funkcijama je u tome što za negativne
ulaze uopće ne aktivira neuron što izuzetno povećava efikasnost. Druga prednost je u tome
što izlaz linearno raste s porastom ulazne vrijednosti što znači da nikad neće ući
u zasićenje. Ta osobina je važna jer utječe na izgled funkcije gubitka te ubrzava
konvergenciju gradijentnog spusta prema minimumu funkcije gubitka \cite{activation_fcn}.


\subsection{Sloj za poduzorkovanje}
\label{sub:pooling}
Sloj za poduzorkovanje (engl. \textit{pooling layer}) služi smanjenju dimenzionalnosti matrice
značajki na izlazu iz konvolucijskog sloja. Najčešće korištene tehnike su maksimalno 
(engl. \textit{max pooling}) i prosječno poduzorkovanje (engl. \textit{average pooling}). 
Radi tako da više 
susjednih vrijednosti spoji u jednu te tako na svom izlazu da matricu manjih dimenzija 
\cite{pooling1}. Na taj način postupno smanjuje broj parametara, smanjuje broj
operacija potrebnih za daljnje računanje te ono najbitnije, kontrolira
prenaučenost \cite{cnn_whatis}. Na slici \ref{pic:pooling} prikazane su obje navedene vrste poduzorkovanja.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{Chapters/neuronska_mreza/CNN/pooling.png} 
    \caption{Poduzorkovanje \cite{pooling1}}
    \label{pic:pooling}
\end{figure}

Prosječno poduzorkovanje izglađuje sliku (engl. \textit{smoothing}). Zbog toga oštri detalji slike
mogu biti izgubljeni što znači da se određene značajke možda neće prepoznati kada se 
koristi ova metoda. Maksimalno uzorkovanje odabire piksele s najvećom vrijednošću iz slike,
a oni su se pokazali kao najbitnije značajke jer daju najbolje rezultate \cite{cnn_whatis}.
Suprotno tome, minimalno uzorkovanje (engl. \textit{min pooling}) odabralo bi piksele s najmanjom
vrijednošću, međutim ono se najrjeđe koristi.

\subsection{Sloj za poravnavanje}
\label{sub:flat}
Sloj za poravnavanje (engl. \textit{flatten layer}) je sloj koji dolazi nakon posljednjeg sloja
za poduzorkovanje. Njegova jedina zadaća je poravnati izlaz iz prijašnjeg sloja.
Ovaj sloj ništa ne računa, niti išta uči, a jedina mu je zadaća od ulaznih mapa značajki
napraviti jedan vektor koji je onda moguće povezati na potpuno povezani sloj. Zbog
toga se često izostavlja iz skica koje prikazuju strukture CNN-ova, kao što je slučaj
na slici \ref{pic:cnn}. Na slici \ref{pic:flatten} prikazana je uloga ovog sloja.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{Chapters/neuronska_mreza/CNN/flatten.png} 
    \caption{Sloj za poravnjavanje \cite{flatten}}
    \label{pic:flatten}
\end{figure}


\subsection{Potpuno povezani sloj}
\label{sub:dense}
Potpuno povezani sloj (engl. \textit{fully connected layer}) detaljnije je pojašnjen u poglavlju
\ref{pog:neuronska_mreza}. Nakon prijašnjih slojeva koji su služili za izvlačenje
značajki iz ulaznih podataka, na red dolazi klasifikacija. Budući da je prethodnik
prvom ovakvom sloju sloj za poravnavanje, ne postoji problem sa spajanjem ovog sloja na dosad
objašnjenu strukturu. Uloga ovog sloja (ili više ovakvih slojeva) je, najjednostavnije
rečeno, klasifikacija. Značajke naučene tijekom konvolucije se ovdje predaju gustoj mreži
neurona koja je sposobna odraditi posao do kraja, tj. naučiti kako različite značajke
pridonose određenoj izlaznoj klasi.

\subsection{Izlazni sloj}
\label{sub:output}
Izlazni (engl. \textit{output layer}) je posljednji sloj potpuno povezanog sloja, a time ujedno i cijele
neuronske mreže. Ima onoliko neurona koliko se želi imati klasa, a pojedina vrijednost
neurona predstavlja vjerojatnost pripadnosti određenoj klasi. Da bi to stvarno funkcioniralo na takav
način, potrebno je odrediti prikladnu aktivacijsku funkciju. Funkcija koja to radi 
naziva se funkcija \textit{softmax}. 
Formalno, \( \text{softmax} : \mathbb{R}^n \to \mathbb{R}^n \),
gdje je \( k \)-ta komponenta izlaznog vektora definirana kao:
\begin{equation}
\text{softmax}_k(x_1, \dots, x_n) = \frac{\exp(x_k)}{\sum_{j} \exp(x_j)}
\end{equation}

Funkcija \textit{softmax} radi dvije ključne stvari:
\begin{itemize}
    \item normalizira sve vrijednosti tako da njihov zbroj bude \( 1 \), tj. izlazni vektor 
        predstavlja distribuciju vjerojatnosti,
    \item pojačava veće vrijednosti (čini ih dominantnijima) i smanjuje manje vrijednosti.
\end{itemize}

Funkcija nosi naziv \textit{softmax} jer odgovara funkciji \( \max \), ali je "meka" u smislu
da je neprekidna i diferencijabilna, za razliku od klasične \( \max \) funkcije 
\cite{snajder2023logreg}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{Chapters/neuronska_mreza/CNN/softmax.png} 
    \caption{Softmax \cite{snajder2023logreg}}
    \label{pic:softmax}
\end{figure}

\subsection{Regularizacija odbacivanjem težina}
Regularizacija je postupak kojim se smanjuje složenost modela kako bi se spriječila
prenaučenost (engl. \textit{overfitting}). Prenaučenost se događa kada model ima preveliku  
sposobnost prilagodbe podacima za treniranje, ali ne i za podacima za testiranje.
U neuronskim mrežama je, uz sloj za poduzorkovanje, implementirana u obliku sloja koji tijekom
treniranja slučajnim odabirom postavlja određeni postotak 
težina u modelu na nulu (engl. \textit{dropout layer}).

\subsection{Poznate arhitekture konvolucijskih mreža}
Arhitektura konvolucijske mreže je ključni faktor koji određuje njene performanse i
učinkovitost. Broj konvolucijskih slojeva, izgled istih (broj filtara, njihova veličina,
pomak), vrsta slojeva za poduzorkovanje te broj i veličina potpuno povezanih slojeva znatno
utječu na brzinu izvođenja i preciznost klasifikacije. Naravno, ne postoji jedan recept koji
najbolje radi na svim vrstama ulaznih podataka, nego različite arhitekture daju bolje rezultate
u određenim situacijama. Dobro je istaknuti određene arhitekture koje imaju povijesno značenje 
zbog toga kako su utjecale na područje istraživanja u dubokom učenju\cite{indian}:

\begin{itemize}
    \item \textbf{LeNet-5 (1998):} 
    CNN sa 7 slojeva dizajnirana za klasifikaciju rukom pisanih brojeva na slikama 
    veličine \(32 \times 32\) piksela u sivim tonovima. Koristila se u bankama za čitanje čekova 
    i bila je prvi značajan korak u korištenju CNN-a u stvarnom svijetu.

    \item \textbf{AlexNet (2012):} 
    Proširena verzija LeNet-a s dubljom arhitekturom (5 konvolucijskih i 3 potpuno povezana 
    sloja). Prva mreža koja je koristila ReLU aktivaciju za brže treniranje. Značajno smanjila
    stopu pogreške na ILSVRC natjecanju i popularizirala duboko učenje.

    \item \textbf{GoogleNet (Inception V1) (2014):} 
    22-slojna mreža s inovativnim \emph{inception module}, koji koristi male konvolucije za 
    smanjenje broja parametara (sa 60 milijuna na samo 4 milijuna). Pobjednik ILSVRC 2014 s 
    top-5 pogreškom manjom od 7\%. Performanse su usporedive s ljudskim prepoznavanjem slika.

    \item \textbf{VGGNet (2014):} 
    Mreža sa 16 konvolucijskih slojeva koja koristi samo \(3 \times 3\) konvolucije s povećanim
    brojem filtara. Iako je jednostavna u dizajnu, ima 138 milijuna parametara, što je čini 
    računalno zahtjevnom za treniranje i implementaciju.
\end{itemize}